\include{preamble} 
\addbibresource{references.bib} % Import the bibliography file
\begin{document}
\include{titlepage}

\newpage

\begin{multicols}{2} % start two-column layout

\section*{Abstract}

In this project, our primary objective is to analyze three gradient-free modifications 
(SGFFW \cite{sahu19a}, FZCGS \cite{gao20b} and ZO-SCGS \cite{lobanov2023})
of the original Frank-Wolfe algorithm. We aim to gain a deep understanding of 
the theory behind these algorithms and to evaluate their performance in a
practical scenario. 

The aforementioned algorithms are specifically designed for constrained 
stochastic optimization problems. They focus on enhancing the iteration complexity, 
which depends on the number of oracle queries, in comparison to existing algorithms. 
Furthermore, the algorithms aspire to be competitive with their first-order counterparts 
in terms of complexity.

Following a theoretical summary of these methods, we conduct practical tests, 
subjecting the algorithms to a \href{https://github.com/IBM/ZOSVRG-BlackBox-Adv}{black-box attacks} 
scenario reported in Section 4.3 \cite{gao20b}.

%\tableofcontents{}


\section{Introduction}

In the examined articles, the minimization constrained optimization 
problem takes one of the following forms:

\begin{enumerate}
    \item \textit{Stochastic} 

    \begin{equation}
    \min _{\mathbf{x} \in \Omega} f(\mathbf{x})=\min _{x \in \Omega} \mathbb{E}_{\mathbf{y} \sim \mathcal{P}}[F(\mathbf{x} ; \mathbf{y})], 
    \label{eq:stochastic}
    \end{equation}

    where $\Omega \in \mathbb{R}^d$ is a closed convex set \cite{sahu19a}, \cite{lobanov2023};
    
    \item \textit{Finite-sum} 
    
    $\min _{\mathbf{x} \in \Omega} F(\mathbf{x})=\dfrac{1}{n} \sum_{i=1}^n f_i(\mathbf{x})$, 
    
    \begin{equation}
        \min _{\mathbf{x} \in \Omega} F(\mathbf{x})=\dfrac{1}{n} \sum_{i=1}^n f_i(\mathbf{x}),
        \label{eq:finite-sum}
    \end{equation}

    where $\Omega \subset \mathbb{R}^d$ denotes a closed convex feasible set \cite{gao20b}.
\end{enumerate}

One of the potential solutions to the problems Eq.~\ref{eq:stochastic}, \ref{eq:finite-sum} 
is the utilization of projection-free methods, such as the Frank-Wolfe algorithm. 
Furthermore, in the papers, they emphasize a stochastic variant of this method that 
relies on a zeroth-order oracle (function queries). Derivative-free optimization finds 
its motivation in scenarios where the analytical form of the function is either 
unavailable or where evaluating the gradient is computationally prohibitive.

Hence, the application of such algorithms is driven by tangible practical benefits. 
In the articles, innovative and more refined modifications are introduced, 
which demand fewer oracle queries to converge to a solution. It's worth noting 
that the initial assumptions about the problem vary slightly. For instance, 
in SGFFW \cite{sahu19a}  and FZCGS \cite{gao20b}, they tackle non-convex smooth
functions, whereas in ZO-SCGS \cite{lobanov2023}, their focus is on convex but non-smooth functions.


\subsection{Frank-Wolfe Algorithm}

The Frank-Wolfe algorithm (first-order) is a versatile optimization method employed in solving 
constrained optimization problems. It is especially suitable for scenarios where 
the constraint set is defined by a large number of linear constraints or 
where projection onto the constraint set is computationally expensive.

The core idea of the Frank-Wolfe algorithm revolves around iteratively 
updating the solution by performing a linear approximation of the objective function. 
The algorithm then proceeds by moving towards a direction that minimizes 
this approximation while ensuring that the solution remains within the constraints. 
This direction is determined by solving a linear optimization subproblem. 
The algorithm converges to the optimal solution by iteratively refining 
the approximation and adjusting the current solution.

\begin{enumerate}
    \item Computation of the gradient of the objective function at the current solution:
       
       \[ \nabla f(x_k) \]
    
    \item Solving a linear optimization subproblem to find a feasible direction \(d_k\) 
          that minimizes the linear approximation of the objective function:
    
       \[ d_k = \arg\min_{d \in \mathcal{C}} \langle \nabla f(x_k), d \rangle \]
    
       where \(\mathcal{C}\) represents the feasible set or constraint set.
    
    \item Updating the current solution \(x_k\) using a step size \(\gamma_k\):
    
       \[ x_{k+1} = x_k + \gamma_k \cdot (d_k - x_k) \]
    \end{enumerate}

    The Frank-Wolfe algorithm is a powerful optimization technique for large-scale 
    constrained problems and an efficient choice for finding solutions 
    while maintaining sparsity and handling complex constraints.

\subsection{Zeroth Order Optimization}

The fundamental idea behind zeroth-order optimization is to efficiently 
explore the function space with minimal reliance on assumptions about 
the function's mathematical properties.
This is achieved through a combination of sampling, interpolation,
and search strategies that guide the optimization process.

When the gradient of a function is not available, we can utilize the difference 
of the function value with respect to two random points to estimate it. 
One well-known method for such estimation, among many others, is the 
coordinate-wise gradient estimator.

\begin{equation}
\hat{\nabla} f(\mathbf{x})=\sum_{j=1}^d \frac{f\left(\mathbf{x}+\mu_j \mathbf{e}_j\right)-f\left(\mathbf{x}-\mu_j \mathbf{e}_j\right)}{2 \mu_j} \mathbf{e}_j,
\label{eq:coorwise-estim}
\end{equation}

where $\mu_j>0$ is the smoothing parameter, and $\mathbf{e}_j \in \mathbb{R}^d$ 
denotes the basis vector where only the $j$-th element is 1 and all the others are 0 . 

The algorithms under investigation incorporate several approaches for 
approximating the gradient.

\medskip
\printbibliography % Print bibliography

\end{multicols}% end two-column layout 
\end{document}

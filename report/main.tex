\include{preamble} 
\addbibresource{references.bib} % Import the bibliography file
\begin{document}
\include{titlepage}

\newpage

\begin{multicols}{2} % start two-column layout

\section*{Abstract}

In this project, our primary objective is to analyze three gradient-free modifications 
(\textit{SGFFW} \cite{sahu19a}, \textit{FZCGS} \cite{gao20b} and \textit{ZO-SCGS} \cite{lobanov2023})
of the original Frank-Wolfe algorithm. We aim to gain a deep understanding of 
the theory behind these algorithms and to evaluate their performance in a
practical scenario. 

The aforementioned algorithms are specifically designed for constrained 
stochastic non-convex optimization problems. They focus on enhancing the iteration complexity, 
which depends on the number of oracle queries, in comparison to existing algorithms. 
Furthermore, the algorithms aspire to be competitive with their first-order counterparts.

Following a theoretical summary of these methods, we conduct practical tests, 
subjecting the algorithms to a \href{https://github.com/IBM/ZOSVRG-BlackBox-Adv}{black-box attacks} 
scenario reported in Section 4.3 \cite{gao20b}.

%\tableofcontents{}


\section{Introduction}

In the examined articles, the minimization constrained optimization 
problem takes one of the following forms:

\begin{enumerate}
    \item \textit{Stochastic} 

    \begin{equation}
    \min _{\mathbf{x} \in \mathcal{C}} f(\mathbf{x})=\min _{x \in \mathcal{C}} \mathbb{E}_{\mathbf{y} \sim \mathcal{P}}[F(\mathbf{x} ; \mathbf{y})], 
    \label{eq:stochastic}
    \end{equation}

    where $\mathcal{C} \in \mathbb{R}^d$ is a closed convex set \cite{sahu19a}, \cite{lobanov2023};
    
    \item \textit{Finite-sum} 
    
    \begin{equation}
        \min _{\mathbf{x} \in \mathcal{C}a} F(\mathbf{x})= \min _{\mathbf{x} \in \mathcal{C}}\dfrac{1}{n} \sum_{i=1}^n f_i(\mathbf{x}),
        \label{eq:finite-sum}
    \end{equation}

    where $\mathcal{C} \subset \mathbb{R}^d$ denotes a closed convex feasible set \cite{gao20b}.
\end{enumerate}

One of the potential solutions to the problems Eq.~\eqref{eq:stochastic}, \eqref{eq:finite-sum} 
is the utilization of projection-free methods, such as the Frank-Wolfe algorithm. 
Furthermore, in the papers, they emphasize a stochastic variant of this method that 
relies on a zeroth-order oracle (function queries). Derivative-free optimization finds 
its motivation in scenarios where the analytical form of the function is either 
unavailable or where evaluating the gradient is computationally prohibitive.

Hence, the application of such algorithms is driven by tangible practical benefits. 
In the articles, innovative and more refined modifications are introduced, 
which demand fewer oracle queries to converge to a solution. It's worth noting 
that the initial assumptions about the problem vary slightly. For instance, 
in \textit{SGFFW} \cite{sahu19a}  and \textit{FZCGS} \cite{gao20b}, they tackle non-convex smooth
functions, whereas in \textit{ZO-SCGS} \cite{lobanov2023}, their focus is on convex but non-smooth functions.


\subsection{Frank-Wolfe Algorithm}

The Frank-Wolfe algorithm (first-order) is a versatile optimization method employed in solving 
constrained optimization problems. It is especially suitable for scenarios where 
the constraint set is defined by a large number of linear constraints or 
where projection onto the constraint set is computationally expensive.

The core idea of the Frank-Wolfe algorithm revolves around iteratively 
updating the solution by performing a linear approximation of the objective function. 
The algorithm then proceeds by moving towards a direction that minimizes 
this approximation while ensuring that the solution remains within the constraints. 
This direction is determined by solving a linear optimization subproblem. 
The algorithm converges to the optimal solution by iteratively refining 
the approximation and adjusting the current solution.

\begin{enumerate}
    \item Computation of the gradient of the objective function at the current solution:
       
       \[ \nabla f(x_k) \]
    
    \item Solving a linear optimization subproblem to find a feasible direction \(d_k\) 
          that minimizes the linear approximation of the objective function:
    
       \[ d_k = \arg\min_{d \in \mathcal{C}} \langle \nabla f(x_k), d \rangle \]
    
       where \(\mathcal{C}\) represents the feasible set or constraint set.
    
    \item Updating the current solution \(x_k\) using a step size \(\gamma_k\):
    
       \[ x_{k+1} = x_k + \gamma_k \cdot (d_k - x_k) \]
    \end{enumerate}

    The Frank-Wolfe algorithm is a powerful optimization technique for large-scale 
    constrained problems and an efficient choice for finding solutions 
    while maintaining sparsity and handling complex constraints.

\subsection{Zeroth Order Optimization}

The fundamental idea behind zeroth-order optimization is to efficiently 
explore the function space with minimal reliance on assumptions about 
the function's mathematical properties.
This is achieved through a combination of sampling, interpolation,
and search strategies that guide the optimization process.

When the gradient of a function is not available, we can utilize the difference 
of the function value with respect to two random points to estimate it. 
One well-known method for such estimation, among many others, is the 
coordinate-wise gradient estimator.

\begin{equation}
\hat{\nabla} f(\mathbf{x})=\sum_{j=1}^d \frac{f\left(\mathbf{x}+\mu_j \mathbf{e}_j\right)-f\left(\mathbf{x}-\mu_j \mathbf{e}_j\right)}{2 \mu_j} \mathbf{e}_j,
\label{eq:coorwise-estim}
\end{equation}

where $\mu_j>0$ is the smoothing parameter, and $\mathbf{e}_j \in \mathbb{R}^d$ 
denotes the basis vector where only the $j$-th element is 1 and all the others are 0 . 

The algorithms under investigation incorporate several approaches for 
approximating the gradient.

\section{SGFFW Algorithm}

The first algorithm that we are studying in our project is 
\textit{Stochastic Gradient-Free Frank-Wolfe (SGFFW)}, which combines 
the principles of stochastic optimization with the Frank-Wolfe framework.

\textit{SGFFW} builds upon the classic Frank-Wolfe algorithm. However, 
instead of relying on full gradients, \textit{SGFFW} uses stochastic 
gradient estimates, making it suitable for large-scale and noisy 
optimization problems.

In the article, \textit{SGFFW} addresses the problem represented by 
eq. \eqref{eq:stochastic}.

In the SGFFW update scheme, the linear minimization and subsequent steps 
differ from those in the ordinary Stochastic Frank-Wolfe method.

\begin{equation}
    \mathbf{d}_t=\left(1-\rho_t\right) \mathbf{d}_{t-1}+\rho_t \mathbf{g}\left(\mathbf{x}_t, \mathbf{y}_t\right)
\label{eq:sahu-d-update}
\end{equation}
    
\begin{equation}
    \mathbf{v}_t=\underset{\mathbf{v} \in \mathcal{C}}{\operatorname{argmin}}\left\langle\mathbf{d}_t, \mathbf{v}\right\rangle 
\label{eq:sahu-v-update}
\end{equation}

\begin{equation}
    \mathbf{x}_{t+1}=\left(1-\gamma_{t+1}\right) \mathbf{x}_t+\gamma_{t+1} \mathbf{v}_t,
\label{eq:sahu-x-update}
\end{equation}

where $g\left(\mathbf{x}_t, \mathbf{y}_t\right)$ is a gradient approximation,
$\mathbf{d}_0=\mathbf{0}$ and $\rho_t$ is a time-decaying sequence.
 


Key characteristics of the algorithm are highlighted as follows:

\begin{itemize}[left=0pt,labelindent=0pt]
    \item A straightforward substitution of $\nabla f(\mathbf{x}_k)$ with 
    its stochastic counterpart, $\nabla F(\mathbf{x}_k ; \mathbf{y}_k)$, 
    carries the potential for divergence, primarily owing to the persistent 
    variance within gradient approximations;
    
    \item The algorithm explores three distinct gradient approximation strategies:
    
    \begin{enumerate}[left=0pt,labelindent=0pt]
        \item \textit{KWSA}
        \small{
        $$
        \mathbf{g}\left(\mathbf{x}_t ; \mathbf{y}\right)=\sum_{i=1}^d \frac{F\left(\mathbf{x}_t+c_t \mathbf{e}_i ; \mathbf{y}\right)-F\left(\mathbf{x}_t ; \mathbf{y}\right)}{c_t} \mathbf{e}_i
        $$
        }
        \item \textit{RDSA}

        Sample $\mathbf{z}_t \sim \mathcal{N}\left(0, \mathbf{I}_d\right)$,
        
        \small{
        $$
        \mathbf{g}\left(\mathbf{x}_t ; \mathbf{y}, \mathbf{z}_t\right)=\frac{F\left(\mathbf{x}_t+c_t \mathbf{z}_t ; \mathbf{y}\right)-F\left(\mathbf{x}_t ; \mathbf{y}\right)}{c_t} \mathbf{z}_t
        $$
        }

        \item \textit{I-RDSA}
        
        Sample $\left\{\mathbf{z}_{i, t}\right\}_{i=1}^m \sim \mathcal{N}\left(0, \mathbf{I}_d\right)$,
        \small{
        $$
        \mathbf{g}\left(\mathbf{x}_t ; \mathbf{y}, \mathbf{z}_t\right)=\frac{1}{m} \sum_{i=1}^m \frac{F\left(\mathbf{x}_t+c_t \mathbf{z}_{i, t} ; \mathbf{y}\right)-F\left(\mathbf{x}_t ; \mathbf{y}\right)}{c_t} \mathbf{z}_{i, t};
        $$
        }
    \end{enumerate}
    
    \item The parameter  $\gamma_t$ are set as $\gamma_t=\dfrac{2}{t+8}$.
\end{itemize}


\subsection{SGFFW: Convergence Analysis}

It emerges that, under certain assumptions, the primal sub-optimality gap 
$\mathbb{E}\left[f\left(\mathbf{x}_t\right)-f\left(\mathbf{x}^*\right)\right]$ 
in the convex case is found to be $O(\dfrac{d^{1/3}}{T^{1/3}})$. 
This matches the performance of the stochastic Frank-Wolfe algorithm, 
which has access to first-order information. The number of queries required by 
stochastic zeroth order oracle to achieve a primal gap of $\epsilon$, i.e., 
$\mathbb{E}\left[f\left(\mathbf{x}_t\right)-f\left(\mathbf{x}^*\right)\right] \leq \epsilon$, 
is given by $O\left(\dfrac{d}{\epsilon^3}\right)$. 

At the same time, in a non-convex scenario, the primal sub-optimality gap and the
number of queries are $O(\dfrac{d^{1/3}}{T^{1/4}})$ and $O\left(\dfrac{d^{4/3}}{\epsilon^4}\right)$, 
respectively.

Hence, the rate of convergence of the proposed algorithm in terms of the 
primal gap is showed to match its first order counterpart in terms
of iterations.

\section{FZCGS Algorithm}

\textit{Faster Zeroth-Order Conditional Gradient Sliding (ZOCGS)} method introduces 
a sliding technique that accelerates convergence by dynamically adjusting 
the step size and direction based on function evaluations. 
\textit{FZOCGS} is capable of handling high-dimensional problems, non-convex functions, 
and noisy objective evaluations, making it suitable for a wide range of applications.

In the article, \textit{ZOCGS} addresses the problem represented by 
eq. \eqref{eq:finite-sum}.


\subsection{FZFW Algorithm}

\textit{Faster Zeroth-Order Frank-Wolfe (FZFW)} method predates \textit{FZCGS} and does not 
incorporate the conditional gradient sliding algorithm.

The update scheme of \textit{FZCGS} consists of the following steps:

\begin{equation}
    \mathbf{u}_k=\arg \max _{\mathbf{u} \in \Omega}\left\langle\mathbf{u},-\hat{\mathbf{v}}_k\right\rangle 
\label{eq:gao-u-update}
\end{equation}
    
\begin{equation}
    \mathbf{d}_k=\mathbf{u}_k-\mathbf{x}_k
\label{eq:gao-d-update}
\end{equation}

\begin{equation}
    \mathbf{x}_{k+1}=\mathbf{x}_k+\gamma_k \mathbf{d}_k,
\label{eq:gao-x-update}
\end{equation}

where $\hat{\mathbf{v}}_k$ is a gradient approximation.

Key highlights of this algorithm encompass the following:

\begin{itemize}[left=0pt,labelindent=0pt]
    
\item The usage of the coordinate-wise gradient estimator (eq. \eqref{eq:coorwise-estim});

\item The gradient estimation occurs at every $q$ iterations are defined as follows:

\small{
\begin{equation}
    \hat{\nabla} f_{S_1}\left(\mathbf{x}_k\right)=\sum_{j=1}^d \frac{f_{S_1}\left(\mathbf{x}_k+\mu_j \mathbf{e}_j\right)-f_{S_1}\left(\mathbf{x}_k-\mu_j \mathbf{e}_j\right)}{2 \mu_j} \mathbf{e}_j,
\label{eq:estim-grad-q}
\end{equation}
}

and at other iterations as follows:

\small{
\begin{equation}
    \hat{\mathbf{v}}_k=\frac{1}{\left|S_2\right|} \sum_{i \in S_2}\left[\hat{\nabla} f_i\left(\mathbf{x}_k\right)-\hat{\nabla} f_i\left(\mathbf{x}_{k-1}\right)+\hat{\mathbf{v}}_{k-1}\right],
\label{eq:estim-grad-not-q}
\end{equation}
}

where $S_1$ and $S_2$ denote the randomly selected samples;
    
\item The estimated number of oracle queries is $O\left(\dfrac{n^{1/2}d}{\epsilon^2}\right)$.
\end{itemize}


\subsection{FZCGS Algorithm}

Although \textit{Faster Zeroth-Order Frank-Wolfe (FZFW)} employs the 
same technique for gradient approximation, it upgrades $\mathbf{x}_{k}$
in a different way via \textit{the conditional gradient sliding algorithm}:

\begin{itemize}[left=0pt,labelindent=0pt]

\item Defining $\phi(\mathbf{y} ; \mathbf{x}, \nabla F(\mathbf{x}), \gamma)=\min _{\mathbf{y} \in \Omega}\langle\nabla F(\mathbf{x}), \mathbf{y}\rangle+$ $\frac{1}{2 \gamma}\|\mathbf{y}-\mathbf{x}\|^2$;

\item Optimizing $\max _{\mathbf{x} \in \Omega}\left\langle\phi^{\prime}\left(\mathbf{u}_t ; \mathbf{u}, \mathbf{g}, \gamma\right), \mathbf{u}_t-\mathbf{x}\right\rangle$,
 which is the Wolfe gap;
 
\item Terminating when the Wolfe gap is smaller than the predefined tolerance $\eta$.

\end{itemize}

\subsection{FZCGS: Convergence Analysis}

Firstly, the utilization of the coordinate-wise gradient estimator significantly 
augments convergence performance in comparison to alternative gradient estimators. 
This estimator also reduces the variance introduced by the randomly selected component 
functions.

Furthermore, it's worth emphasizing that the incorporation of 
\textit{conditional gradient sliding} has yielded a substantial 
reduction in the algorithm's iteration complexity, transitioning from 
$O\left(\dfrac{n^{1/2}d}{\epsilon^2}\right)$
to $O\left(\dfrac{n^{1/2}d}{\epsilon}\right)$.

In conclusion, it has been substantiated, through theoretical analysis, 
that \textit{FZCGS} demonstrates superior convergence rates when 
compared to previously developed methods designed for non-convex optimization. 
Remarkably, its iteration complexity even outperforms that of its first-order counterparts.

\section{ZO-SCGS Algorithm}

\subsection{ZO-SCGS: Convergence Analysis}
 
\medskip
\printbibliography % Print bibliography

\end{multicols}% end two-column layout 
\end{document}
 
  















